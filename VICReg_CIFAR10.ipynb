{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd933ab8",
   "metadata": {},
   "source": [
    "# Trying VICReg on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5bf3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from os import path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "data_root = '/home/fcfschulz/Documents/workspace/data/Vision/torchvision_ds/'\n",
    "save_root = './results/vic_reg/'\n",
    "\n",
    "### For COLAB ############################################################\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#\n",
    "#data_root = './'\n",
    "#save_root ='./drive/MyDrive/Colab_Notebooks/PhD/SSL_pytorch/results/vic_reg/'\n",
    "#\n",
    "# !cp -r ./drive/MyDrive/Colab_Notebooks/PhD/SSL_pytorch/* ./\n",
    "##########################################################################\n",
    "\n",
    "from augmentation import VICReg_augmentaions\n",
    "from models import VICReg\n",
    "from optimizer import LARS\n",
    "from utils import check_existing_model, Linear_Protocoler\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Print Device Type\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Program running on {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"Program running on CPU\")\n",
    "\n",
    "    \n",
    "# Define hyperparameters\n",
    "dl_kwargs = {'batch_size': 512, 'shuffle': True, 'num_workers': 2}\n",
    "\n",
    "optim_params = {'lr':0.2 * dl_kwargs['batch_size']/256,\n",
    "                'weight_decay': 1.5e-6,\n",
    "                'exclude_bias_and_norm':True}\n",
    "train_params = {'num_epochs': 400, 'warmup_epchs': 10, 'eta_min':1e-3}\n",
    "eval_params  = {'lr':1e-2, 'num_epochs': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7fa78",
   "metadata": {},
   "source": [
    "# VICReg\n",
    "### Define Augmentation and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e82d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cifar10 Mean and Std\n",
    "CIFAR10_NORM = [[0.4914, 0.4822, 0.4465],[0.2023, 0.1994, 0.2010]]\n",
    "\n",
    "# Define Augmentations\n",
    "train_transf = VICReg_augmentaions(image_size=32, normalize=CIFAR10_NORM)\n",
    "\n",
    "train_eval_transf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, (0.8, 1.),interpolation=transforms.InterpolationMode('bicubic')),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(*CIFAR10_NORM)])\n",
    "\n",
    "test_transf = transforms.Compose([transforms.ToTensor(), transforms.Normalize(*CIFAR10_NORM)])\n",
    "\n",
    "# Define Datasets\n",
    "train_ds = CIFAR10(root=data_root, train = True, download = True, transform = train_transf)\n",
    "train_eval_ds = CIFAR10(root=data_root, train = True, transform = train_eval_transf, download = True)\n",
    "test_ds  = CIFAR10(root=data_root, train = False, transform = test_transf, download = True)\n",
    "\n",
    "# Define Dataloaders\n",
    "train_dl = DataLoader(train_ds, drop_last=True, **dl_kwargs)\n",
    "train_eval_dl = DataLoader(train_eval_ds, drop_last=False, **dl_kwargs)\n",
    "test_dl  = DataLoader(test_ds, drop_last=False, **dl_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceae48d",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04393c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = resnet18(zero_init_residual=True)\n",
    "repre_dim = resnet.fc.in_features\n",
    "\n",
    "model = VICReg(resnet, projector_hidden = (2048,2048)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2858b2",
   "metadata": {},
   "source": [
    "### Define Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe581b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = LARS(model.parameters(), **optim_params)\n",
    "\n",
    "# Define scheduler for warmup\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lambda it : (it+1)/(train_params['warmup_epchs']*len(train_dl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b717a2",
   "metadata": {},
   "source": [
    "### Check for trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77c1d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Init\n",
    "lp_acc = []\n",
    "loss_hist = []\n",
    "lr_hist = []\n",
    "\n",
    "epoch_start, saved_data = check_existing_model(save_root, device)\n",
    "if saved_data:\n",
    "    # Extract data\n",
    "    model.load_state_dict(saved_data['model'])\n",
    "    optimizer.load_state_dict(saved_data['optim'])\n",
    "    if epoch_start >= train_params['warmup_epchs']:\n",
    "        iters_left = (train_params['num_epochs']-train_params['warmup_epchs'])*len(train_dl)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                   iters_left,\n",
    "                                                   eta_min=train_params['eta_min'])\n",
    "    scheduler.load_state_dict(saved_data['sched'])\n",
    "    lp_acc = saved_data['lp_acc']\n",
    "    loss_hist = saved_data['loss_hist']\n",
    "    lr_hist = saved_data['lr_hist']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd25bc1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear protocol for random init model\n",
    "if len(lp_acc)==0:\n",
    "    linear_proto = Linear_Protocoler(model.backbone_net, out_dim=repre_dim)\n",
    "    linear_proto.train(train_eval_dl, eval_params['num_epochs'], eval_params['lr'])\n",
    "    lp_acc.append(linear_proto.get_accuracy(test_dl))\n",
    "    print(f'Accuracy after init: {100*lp_acc[-1]}')\n",
    "    \n",
    "# get total number of iterations\n",
    "total_iters = train_params['num_epochs'] * len(train_dl)\n",
    "\n",
    "# Run Training\n",
    "for epoch in range(epoch_start, train_params['num_epochs']):\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "    for (x1,x2), _ in train_dl:\n",
    "        x1,x2 = x1.to(device), x2.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = model(x1,x2)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Scheduler every iteration for cosine deday\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save loss and LR\n",
    "        epoch_loss += loss.item()\n",
    "        lr_hist.extend(scheduler.get_last_lr())\n",
    "    \n",
    "    # Switch to Cosine Decay after warmup period\n",
    "    if epoch+1==train_params['warmup_epchs']:\n",
    "        iters_left = (train_params['num_epochs']-train_params['warmup_epchs'])*len(train_dl)\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                       iters_left,\n",
    "                                                       eta_min=train_params['eta_min'])\n",
    "    \n",
    "    # Log\n",
    "    loss_hist.append(epoch_loss/len(train_dl))\n",
    "    print(f'Epoch: {epoch}, Loss: {loss_hist[-1]}, Time epoch: {time.time() - start_time}')\n",
    "    \n",
    "    # Run linear protocol and save stats\n",
    "    if (epoch+1)%5==0:\n",
    "        # Linear protocol\n",
    "        linear_proto = Linear_Protocoler(model.backbone_net, out_dim=repre_dim)\n",
    "        linear_proto.train(train_eval_dl, eval_params['num_epochs'], eval_params['lr'])\n",
    "        lp_acc.append(linear_proto.get_accuracy(test_dl))\n",
    "        print(f'Accuracy after epoch {epoch}: {100*lp_acc[-1]}')\n",
    "        \n",
    "        torch.save({'model':model.state_dict(),\n",
    "                    'optim': optimizer.state_dict(),\n",
    "                    'sched': scheduler.state_dict(),\n",
    "                    'lp_acc': lp_acc,\n",
    "                    'loss_hist': loss_hist,\n",
    "                    'lr_hist': lr_hist}, \n",
    "                   path.join(save_root, f'epoch_{epoch+1:03}.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f8461",
   "metadata": {},
   "source": [
    "### Visualize loss, eval accuracy and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(16,4))\n",
    "ax1.plot(loss_hist)\n",
    "ax1.set_title(\"Loss\")\n",
    "\n",
    "ax2.plot(lp_acc)\n",
    "ax2.set_title(\"Linear Evaluation Protocol\")\n",
    "\n",
    "ax3.plot(lr_hist)\n",
    "ax3.set_title(\"Learning rate\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43ed0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741c57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3ef17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4ba4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Playground)",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
