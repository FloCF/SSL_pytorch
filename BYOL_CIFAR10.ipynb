{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd933ab8",
   "metadata": {},
   "source": [
    "# Trying Bootstrap Your Own Latent (BYOL) on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c5bf3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program running on CPU\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "from os import listdir, makedirs, path\n",
    "from typing import Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from augmentation import BYOL_transform\n",
    "from models import BYOL\n",
    "from optimizer import LARS\n",
    "from utils import Linear_Protocoler\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Print Device Type\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Program running on {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    print(\"Program running on CPU\")\n",
    "\n",
    "# Define hyperparameters\n",
    "data_root = '/home/fcfschulz/Documents/workspace/data/Vision/torchvision_ds/'\n",
    "save_root = '/home/fcfschulz/Documents/workspace/data/saved_models/byol_cifar10/'\n",
    "\n",
    "### For COLAB ############################################################\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#\n",
    "#data_root = './'\n",
    "#save_root ='./drive/MyDrive/Colab_Notebooks/PhD/data/saved_models/byol/'\n",
    "##########################################################################\n",
    "\n",
    "# Make save root\n",
    "makedirs(save_root, exist_ok=True)\n",
    "\n",
    "dl_kwargs = {'batch_size': 256, 'shuffle': True, 'num_workers': 2}\n",
    "\n",
    "optim_params = {'lr':0.2, 'weight_decay': 1.5e-6,'exclude_bias_and_norm':True}\n",
    "train_params = {'num_epochs': 250, 'warmup_epchs': 10}\n",
    "\n",
    "eval_params = {'lr':1e-2, 'num_epochs': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed7fa78",
   "metadata": {},
   "source": [
    "# BYOL\n",
    "### Define Augmentation and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6e82d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Define Augmentations\n",
    "train_transf = BYOL_transform(image_size=32, normalize=(0.5,0.5))\n",
    "\n",
    "train_eval_transf = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32, (0.8, 1.),interpolation=transforms.InterpolationMode('bicubic')),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(0.5,0.5)])\n",
    "\n",
    "test_transf = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0.5,0.5)])\n",
    "\n",
    "# Define Datasets\n",
    "train_ds = CIFAR10(root=data_root, train = True, download = True, transform = train_transf)\n",
    "train_eval_ds = CIFAR10(root=data_root, train = True, transform = train_eval_transf, download = True)\n",
    "test_ds  = CIFAR10(root=data_root, train = False, transform = test_transf, download = True)\n",
    "\n",
    "# Define Dataloaders\n",
    "train_dl = DataLoader(train_ds, drop_last=True, **dl_kwargs)#\n",
    "train_eval_dl = DataLoader(train_eval_ds, drop_last=False, **dl_kwargs)\n",
    "test_dl  = DataLoader(test_ds, drop_last=False, **dl_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceae48d",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fee883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_net = resnet18()\n",
    "repre_dim = backbone_net.fc.in_features\n",
    "backbone_net.fc = nn.Flatten()\n",
    "\n",
    "byol = BYOL(backbone_net, repre_dim, 32, 1024).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2858b2",
   "metadata": {},
   "source": [
    "### Define Optimizer & Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64fe581b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = LARS(byol.parameters(), **optim_params)\n",
    "\n",
    "# Define scheduler for warmup\n",
    "scheduler = opt.lr_scheduler.LambdaLR(optimizer, lambda it : (it+1)/(train_params['warmup_epchs']*len(train_dl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b717a2",
   "metadata": {},
   "source": [
    "### Check for trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77c1d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Init\n",
    "epoch_start = 0\n",
    "lp_acc = []\n",
    "loss_hist = []\n",
    "lr_hist = []\n",
    "tau_hist = []\n",
    "\n",
    "if path.exists(path.join(save_root, f'epoch_{5:03}.tar')):\n",
    "    user_answer = \"Users_answer\"\n",
    "    while user_answer not in [\"y\",\"n\"]:\n",
    "        user_answer = input(\"Pretrained model available, use it?[y/n]: \").lower()[0]\n",
    "    if user_answer==\"y\":\n",
    "        epoch_start = max([int(file[-7:-4]) for file in listdir(save_root)])\n",
    "        # Load data\n",
    "        saved_data = torch.load(path.join(save_root, f'epoch_{epoch_start:03}.tar'), map_location=device)\n",
    "        # Extract data\n",
    "        byol.load_state_dict(saved_data['model'])\n",
    "        optimizer.load_state_dict(saved_data['optim'])\n",
    "        if epoch_start >= train_params['warmup_epchs']:\n",
    "            iters_left = (train_params['num_epochs']-train_params['warmup_epchs'])*len(train_dl)\n",
    "            scheduler = opt.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                       iters_left,\n",
    "                                                       eta_min=train_params['eta_min'])\n",
    "        scheduler.load_state_dict(saved_data['sched'])\n",
    "        lp_acc = saved_data['lp_acc']\n",
    "        loss_hist = saved_data['loss_hist']\n",
    "        lr_hist = saved_data['lr_hist']\n",
    "        tau_hist = saved_data['tau_hist']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd25bc1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfbc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear protocol for random init model\n",
    "if len(lp_acc)==0:\n",
    "    linear_proto = Linear_Protocoler(byol.backbone_net, out_dim=repre_dim)\n",
    "    linear_proto.train(train_eval_dl, eval_params['num_epochs'], eval_params['base_lr'])\n",
    "    lp_acc.append(linear_pro.get_accuracy(test_dl))\n",
    "    \n",
    "# get total number of iterations\n",
    "total_iters = train_params['num_epochs'] * len(train_dl)\n",
    "\n",
    "# Run Training\n",
    "for epoch in range(epoch_start, train_params['num_epochs']):\n",
    "    epoch_loss = 0\n",
    "    start_time = time.time()\n",
    "    for i, ((x1,x2), _) in enumerate(train_dl):\n",
    "        x1,x2 = x1.to(device), x2.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        loss = byol(x1,x2)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update momentum encoder\n",
    "        # get τ\n",
    "        τ = byol.get_tau(1+i+len(train_dl)*epoch,total_iters)\n",
    "        byol.update_moving_average(τ)\n",
    "        tau_hist.append(τ)\n",
    "        \n",
    "        # Scheduler every iteration for cosine deday\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save loss and LR\n",
    "        epoch_loss += loss.item()\n",
    "        lr_hist.extend(scheduler.get_last_lr())\n",
    "    \n",
    "    # Switch to Cosine Decay after warmup period\n",
    "    if epoch+1==train_params['warmup_epchs']:\n",
    "        iters_left = (train_params['num_epochs']-train_params['warmup_epchs'])*len(train_dl)\n",
    "        scheduler = opt.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                       iters_left,\n",
    "                                                       eta_min=train_params['eta_min'])\n",
    "    \n",
    "    # Log\n",
    "    loss_hist.append(epoch_loss/len(train_dl))\n",
    "    print(f'Epoch: {epoch}, Loss: {loss_hist[-1]}, Time epoch: {time.time() - start_time}')\n",
    "    \n",
    "    # Run linear protocol and save stats\n",
    "    if (epoch+1)%5==0:\n",
    "        # Linear protocol\n",
    "        linear_proto = Linear_Protocoler(byol.backbone_net, out_dim=repre_dim)\n",
    "        linear_proto.train(train_eval_dl, eval_params['num_epochs'], eval_params['base_lr'])\n",
    "        lp_acc.append(linear_pro.get_accuracy(test_dl))\n",
    "        \n",
    "        torch.save({'model':byol.state_dict(),\n",
    "                    'optim': optimizer.state_dict(),\n",
    "                    'sched': scheduler.state_dict(),\n",
    "                    'lp_acc': lp_acc,\n",
    "                    'loss_hist': loss_hist,\n",
    "                    'lr_hist': lr_hist,\n",
    "                    'tau_hist': tau_hist}, \n",
    "                   path.join(save_root, f'epoch_{epoch+1:03}.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f8461",
   "metadata": {},
   "source": [
    "### Visualize loss, learning rate and $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e23f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16,4))\n",
    "ax1.plot(loss_hist)\n",
    "ax1.set_title(\"Loss\")\n",
    "\n",
    "ax2.plot(lp_acc)\n",
    "ax2.set_title(\"Linear Evaluation Protocol\")\n",
    "\n",
    "ax3.plot(lr_hist)\n",
    "ax3.set_title(\"Learning rate\")\n",
    "\n",
    "ax4.plot(tau_hist)\n",
    "ax4.set_title(\"Tau\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14cf40",
   "metadata": {},
   "source": [
    "### Colab transfer saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#!cp ./saved/* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741c57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d3ef17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4ba4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Playground)",
   "language": "python",
   "name": "playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
